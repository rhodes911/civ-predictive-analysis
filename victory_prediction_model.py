#!/usr/bin/env python3
"""
Victory Prediction Model Training Script
=======================================

This script trains a Random Forest classifier to predict Civ VI game winners
based on Turn 20 performance metrics.

Prerequisites:
    - training_data.csv (generated by ml_data_preparation.py)
    - Required packages: scikit-learn, pandas, numpy, matplotlib, seaborn

Usage:
    python victory_prediction_model.py

Output:
    - trained_model.pkl: Saved Random Forest model
    - feature_importance.csv: Feature importance rankings
    - model_performance_report.txt: Detailed evaluation metrics
    - confusion_matrix.png: Visualization of model performance
"""

import pandas as pd
import numpy as np
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import os

from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (classification_report, confusion_matrix, 
                           accuracy_score, precision_score, recall_score, f1_score)
from sklearn.preprocessing import StandardScaler

class VictoryPredictionModel:
    def __init__(self):
        """Initialize the victory prediction model trainer"""
        self.model = None
        self.scaler = StandardScaler()
        self.feature_columns = [
            # Raw metrics
            'num_cities', 'population', 'techs', 'civics',
            'yields_science', 'yields_culture', 'yields_production',
            'buildings', 'districts', 'current_score', 'game_turn',
            # Calculated features
            'science_per_city', 'population_per_city', 'buildings_per_city',
            'development_index',
            # Relative rankings (most important!)
            'science_rank', 'cities_rank', 'score_rank', 'population_rank'
        ]
        
    def load_training_data(self):
        """Load and validate training data"""
        try:
            if not os.path.exists('training_data.csv'):
                print("‚ùå training_data.csv not found!")
                print("   Please run ml_data_preparation.py first")
                return None
                
            df = pd.read_csv('training_data.csv')
            print(f"‚úÖ Loaded training data: {len(df)} records")
            
            # Validate required columns exist
            missing_cols = [col for col in self.feature_columns if col not in df.columns]
            if missing_cols:
                print(f"‚ùå Missing required columns: {missing_cols}")
                return None
                
            # Check for target column
            if 'will_win' not in df.columns:
                print("‚ùå Target column 'will_win' not found")
                return None
                
            return df
            
        except Exception as e:
            print(f"‚ùå Error loading training data: {e}")
            return None
    
    def prepare_features(self, df):
        """Prepare features for model training"""
        print("\nüîß Preparing features for model training...")
        
        # Handle any missing values
        df_clean = df.copy()
        for col in self.feature_columns:
            if df_clean[col].isnull().any():
                median_val = df_clean[col].median()
                df_clean[col].fillna(median_val, inplace=True)
                print(f"   Filled {df_clean[col].isnull().sum()} missing values in {col}")
        
        # Extract features and target
        X = df_clean[self.feature_columns]
        y = df_clean['will_win']
        
        print(f"‚úÖ Features prepared: {X.shape[0]} samples, {X.shape[1]} features")
        print(f"   Target distribution: {y.sum()} winners, {len(y) - y.sum()} non-winners")
        
        return X, y, df_clean
    
    def train_model(self, X, y):
        """Train Random Forest model with hyperparameter tuning"""
        print("\nü§ñ Training Random Forest model...")
        
        # Split data for training and testing
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42, stratify=y
        )
        
        print(f"   Training set: {len(X_train)} samples")
        print(f"   Test set: {len(X_test)} samples")
        
        # Define hyperparameter grid for tuning
        param_grid = {
            'n_estimators': [50, 100, 200],
            'max_depth': [5, 10, 15, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4]
        }
        
        # Initialize Random Forest
        rf = RandomForestClassifier(random_state=42)
        
        # Perform grid search with cross-validation
        print("   Performing hyperparameter tuning...")
        grid_search = GridSearchCV(
            rf, param_grid, cv=5, scoring='accuracy', 
            n_jobs=-1, verbose=0
        )
        
        grid_search.fit(X_train, y_train)
        
        # Get best model
        self.model = grid_search.best_estimator_
        
        print(f"‚úÖ Best parameters: {grid_search.best_params_}")
        print(f"‚úÖ Cross-validation accuracy: {grid_search.best_score_:.3f}")
        
        return X_train, X_test, y_train, y_test
    
    def evaluate_model(self, X_train, X_test, y_train, y_test):
        """Comprehensive model evaluation"""
        print("\nüìä Evaluating model performance...")
        
        # Make predictions
        y_train_pred = self.model.predict(X_train)
        y_test_pred = self.model.predict(X_test)
        
        # Calculate metrics
        train_accuracy = accuracy_score(y_train, y_train_pred)
        test_accuracy = accuracy_score(y_test, y_test_pred)
        test_precision = precision_score(y_test, y_test_pred)
        test_recall = recall_score(y_test, y_test_pred)
        test_f1 = f1_score(y_test, y_test_pred)
        
        print(f"   Training Accuracy: {train_accuracy:.3f}")
        print(f"   Test Accuracy: {test_accuracy:.3f}")
        print(f"   Test Precision: {test_precision:.3f}")
        print(f"   Test Recall: {test_recall:.3f}")
        print(f"   Test F1-Score: {test_f1:.3f}")
        
        # Check for overfitting
        if train_accuracy - test_accuracy > 0.1:
            print("‚ö†Ô∏è  Warning: Possible overfitting detected")
        else:
            print("‚úÖ Good generalization - no significant overfitting")
        
        # Detailed classification report
        print("\nüìã Detailed Classification Report:")
        print(classification_report(y_test, y_test_pred, 
                                  target_names=['Non-Winner', 'Winner']))
        
        return {
            'train_accuracy': train_accuracy,
            'test_accuracy': test_accuracy,
            'test_precision': test_precision,
            'test_recall': test_recall,
            'test_f1': test_f1,
            'y_test': y_test,
            'y_test_pred': y_test_pred
        }
    
    def analyze_feature_importance(self):
        """Analyze and visualize feature importance"""
        print("\nüéØ Analyzing feature importance...")
        
        # Get feature importances
        importances = self.model.feature_importances_
        feature_importance_df = pd.DataFrame({
            'feature': self.feature_columns,
            'importance': importances
        }).sort_values('importance', ascending=False)
        
        print("   Top 10 Most Important Features:")
        for i, (_, row) in enumerate(feature_importance_df.head(10).iterrows(), 1):
            print(f"   {i:2d}. {row['feature']:20s} ({row['importance']:.3f})")
        
        # Save feature importance
        feature_importance_df.to_csv('feature_importance.csv', index=False)
        print("‚úÖ Feature importance saved to: feature_importance.csv")
        
        return feature_importance_df
    
    def create_confusion_matrix_plot(self, y_test, y_test_pred):
        """Create and save confusion matrix visualization"""
        print("\nüìà Creating confusion matrix visualization...")
        
        try:
            # Create confusion matrix
            cm = confusion_matrix(y_test, y_test_pred)
            
            # Create plot
            plt.figure(figsize=(8, 6))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                       xticklabels=['Non-Winner', 'Winner'],
                       yticklabels=['Non-Winner', 'Winner'])
            plt.title('Victory Prediction Model - Confusion Matrix')
            plt.xlabel('Predicted')
            plt.ylabel('Actual')
            
            # Add percentage annotations
            total = cm.sum()
            for i in range(2):
                for j in range(2):
                    percentage = cm[i, j] / total * 100
                    plt.text(j + 0.5, i + 0.3, f'({percentage:.1f}%)', 
                            ha='center', va='center', fontsize=10)
            
            plt.tight_layout()
            plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')
            print("‚úÖ Confusion matrix saved to: confusion_matrix.png")
            
        except Exception as e:
            print(f"‚ö†Ô∏è  Could not create confusion matrix plot: {e}")
            print("   (matplotlib/seaborn may not be available)")
    
    def save_model(self):
        """Save the trained model"""
        try:
            with open('trained_model.pkl', 'wb') as f:
                pickle.dump(self.model, f)
            print("‚úÖ Trained model saved to: trained_model.pkl")
            return True
        except Exception as e:
            print(f"‚ùå Error saving model: {e}")
            return False
    
    def generate_performance_report(self, metrics, feature_importance_df, df):
        """Generate comprehensive performance report"""
        report = []
        report.append("üéØ CIV VI VICTORY PREDICTION MODEL PERFORMANCE REPORT")
        report.append("=" * 65)
        report.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # Model overview
        report.append("ü§ñ MODEL OVERVIEW:")
        report.append(f"Model Type: Random Forest Classifier")
        report.append(f"Training Data: {len(df)} civilization records")
        report.append(f"Features Used: {len(self.feature_columns)} features")
        report.append(f"Target: Binary classification (will_win: 0/1)")
        report.append("")
        
        # Performance metrics
        report.append("üìä PERFORMANCE METRICS:")
        report.append(f"Test Accuracy: {metrics['test_accuracy']:.1%}")
        report.append(f"Precision: {metrics['test_precision']:.1%}")
        report.append(f"Recall: {metrics['test_recall']:.1%}")
        report.append(f"F1-Score: {metrics['test_f1']:.1%}")
        report.append("")
        
        # Interpretation
        report.append("üìù PERFORMANCE INTERPRETATION:")
        if metrics['test_accuracy'] >= 0.80:
            report.append("üü¢ EXCELLENT: Model achieves high accuracy in predicting winners")
        elif metrics['test_accuracy'] >= 0.70:
            report.append("üü° GOOD: Model shows solid predictive capability")
        elif metrics['test_accuracy'] >= 0.60:
            report.append("üü† FAIR: Model has some predictive value but could be improved")
        else:
            report.append("üî¥ POOR: Model needs significant improvement")
        
        if metrics['test_precision'] >= 0.75:
            report.append("‚úÖ High precision: When model predicts a winner, it's usually correct")
        else:
            report.append("‚ö†Ô∏è  Moderate precision: Some false positive predictions")
            
        if metrics['test_recall'] >= 0.75:
            report.append("‚úÖ High recall: Model catches most actual winners")
        else:
            report.append("‚ö†Ô∏è  Moderate recall: Model misses some actual winners")
        
        report.append("")
        
        # Feature importance
        report.append("üéØ TOP 10 MOST IMPORTANT FEATURES:")
        for i, (_, row) in enumerate(feature_importance_df.head(10).iterrows(), 1):
            importance_pct = row['importance'] * 100
            report.append(f"{i:2d}. {row['feature']:25s} {importance_pct:5.1f}%")
        report.append("")
        
        # Strategic insights
        report.append("üí° STRATEGIC INSIGHTS:")
        top_feature = feature_importance_df.iloc[0]['feature']
        top_importance = feature_importance_df.iloc[0]['importance'] * 100
        
        if 'rank' in top_feature:
            report.append(f"üî• RELATIVE POSITIONING IS KEY: {top_feature} ({top_importance:.1f}% importance)")
            report.append("   Strategy: Focus on achieving top rankings early rather than raw numbers")
        
        if any('science' in feat for feat in feature_importance_df.head(5)['feature']):
            report.append("üß™ SCIENCE ADVANTAGE: Science-related metrics highly predictive")
            report.append("   Strategy: Prioritize research and science infrastructure")
        
        if any('cities' in feat for feat in feature_importance_df.head(5)['feature']):
            report.append("üèôÔ∏è  EXPANSION MATTERS: City count/development highly predictive")
            report.append("   Strategy: Balance expansion with development")
        
        report.append("")
        
        # Actionable recommendations
        report.append("üöÄ ACTIONABLE RECOMMENDATIONS:")
        report.append("1. Use this model for Turn 20+ predictions in current games")
        report.append("2. Focus early game strategy on the top 5 features identified")
        report.append("3. Monitor relative rankings vs absolute values")
        report.append("4. Collect more game data to improve model accuracy over time")
        report.append("")
        
        # Technical details
        report.append("‚öôÔ∏è  TECHNICAL DETAILS:")
        report.append(f"Random Forest Parameters:")
        if hasattr(self.model, 'n_estimators'):
            report.append(f"  Trees: {self.model.n_estimators}")
            report.append(f"  Max Depth: {self.model.max_depth}")
            report.append(f"  Min Samples Split: {self.model.min_samples_split}")
        
        # Save report
        report_text = "\n".join(report)
        with open('model_performance_report.txt', 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        print("\n" + report_text)
        print(f"\nüìã Full report saved to: model_performance_report.txt")
    
    def run_training(self):
        """Run the complete model training process"""
        print("üöÄ Starting Victory Prediction Model Training")
        print("=" * 50)
        
        # Step 1: Load training data
        df = self.load_training_data()
        if df is None:
            return False
        
        # Step 2: Prepare features
        X, y, df_clean = self.prepare_features(df)
        
        # Step 3: Train model
        X_train, X_test, y_train, y_test = self.train_model(X, y)
        
        # Step 4: Evaluate model
        metrics = self.evaluate_model(X_train, X_test, y_train, y_test)
        
        # Step 5: Analyze feature importance
        feature_importance_df = self.analyze_feature_importance()
        
        # Step 6: Create visualizations
        self.create_confusion_matrix_plot(metrics['y_test'], metrics['y_test_pred'])
        
        # Step 7: Save model
        self.save_model()
        
        # Step 8: Generate comprehensive report
        self.generate_performance_report(metrics, feature_importance_df, df_clean)
        
        print("\nüéâ Model training completed successfully!")
        print("Generated files:")
        print("  üì¶ trained_model.pkl - Saved Random Forest model")
        print("  üìä feature_importance.csv - Feature rankings")
        print("  üìã model_performance_report.txt - Detailed analysis")
        print("  üìà confusion_matrix.png - Performance visualization")
        print("\nNext step: Run predict_winner.py to make predictions!")
        
        return True

if __name__ == "__main__":
    # Run the model training process
    trainer = VictoryPredictionModel()
    trainer.run_training()
